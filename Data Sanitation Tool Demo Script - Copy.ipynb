{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This file contains the data sanitation dashboard app code\"\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import base64\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import dash_table\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash_extensions import Download\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input,Output\n",
    "from dash_extensions.snippets import send_data_frame\n",
    "\n",
    "external_stylesheets = [dbc.themes.BOOTSTRAP]\n",
    "\n",
    "app = dash.Dash(\n",
    "    __name__,\n",
    "    meta_tags=[{\"name\": \"viewport\", \"content\": \"width=device-width, initial-scale=1\"}],\n",
    "    external_stylesheets=external_stylesheets\n",
    ")\n",
    "app.title = \"Data Sanitation Tool\"\n",
    "\n",
    "server = app.server\n",
    "app.config.suppress_callback_exceptions = True\n",
    "\n",
    "tab_style = {\n",
    "    'borderTop': '2px solid #ffaf2a',\n",
    "}\n",
    "\n",
    "# App libraries\n",
    "from data_input.read_system_info import gather_inputs\n",
    "from data_input.read_meteo_data import read_weather_data\n",
    "from data_input.read_operational_data import read_inverter_data\n",
    "from data_input.poa_irradiance import get_operational_irradiance\n",
    "\n",
    "from data_sanitization.utc import get_tz\n",
    "from data_sanitization.models import predict_missing_data\n",
    "from data_sanitization.site_location_pvlib import get_site_location\n",
    "from data_sanitization.clear_sky_irradiance import clearsky_irradiance\n",
    "from data_sanitization.eliminate_night_values import eliminate_nightvalues\n",
    "from data_sanitization.filtering import multiindex_irradiance_filter\n",
    "from data_sanitization.filtering import multiindex_current_filter\n",
    "from data_sanitization.filtering import multiindex_voltage_filter\n",
    "\n",
    "# from data_sanitization.plot_graph import plot_data_analysis_graph\n",
    "# from data_sanitization.plot_graph import input_data_summary\n",
    "# from data_sanitization.misc_func import fig_to_uri\n",
    "from data_sanitization.misc_func import data_summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_summary(array_info, df_in):\n",
    "    df = df_in.drop('P', level='curve', axis=1).copy()\n",
    "\n",
    "    input_data_summary = pd.DataFrame(index=array_info['input_name'].values,\n",
    "                                      columns=['Missing/Bad', 'Available'])\n",
    "    for i, j in array_info.index:\n",
    "        input_data_summary['Missing/Bad'].loc[i + '-' + j] = (\n",
    "            round((df[i][j].isna().sum().mean() / len(df)) * 100, 2))\n",
    "    input_data_summary['Available'] = 100 - input_data_summary['Missing/Bad']\n",
    "\n",
    "    # Creating Stacked Bar plot using data\n",
    "    fig = go.Figure(data=[go.Bar(name='Available',\n",
    "                                 x=input_data_summary.index,\n",
    "                                 y=input_data_summary['Available'],\n",
    "                                 marker_color='#FF8800'),\n",
    "\n",
    "                          go.Bar(name='Missing/Outlier',\n",
    "                                 x=input_data_summary.index,\n",
    "                                 y=input_data_summary['Missing/Bad'],\n",
    "                                 marker_color='#636EFA')],\n",
    "\n",
    "                    layout=go.Layout(xaxis=dict(title=\"Inputs\"),\n",
    "                                     yaxis=dict(title=\"Data %\"),\n",
    "                                     template='plotly_white',\n",
    "                                     barmode=\"stack\",\n",
    "                                     autosize=False,\n",
    "                                     width=1000,\n",
    "                                     height=500))\n",
    "\n",
    "    fig.update_layout(margin=dict(l=20, r=10, t=20, b=20),\n",
    "                      hoverlabel=dict(bgcolor=\"white\",\n",
    "                                      font_size=16,\n",
    "                                      font_family=\"Roboto\"),\n",
    "                      legend=dict(orientation=\"h\", yanchor=\"top\",\n",
    "                                  y=1.02, xanchor=\"left\", x=0.33))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_data_analysis_graph(inverter_data, inverter_data_sanitized, inv_name,\n",
    "                             variable, ylabel, title):\n",
    "    df = pd.DataFrame()\n",
    "    inverter, mppt = inv_name.split('-')[0], inv_name.split('-')[1]\n",
    "    input_name = inv_name + '-' + variable\n",
    "\n",
    "    df['Pre Sanitation'] = inverter_data[inverter][mppt][variable]\n",
    "    df['Post Sanitation'] = inverter_data_sanitized[inverter][mppt][variable]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    dash_obj1 = go.Scatter(x=df.index,\n",
    "                           y=df['Pre Sanitation'],\n",
    "                           name='Pre Sanitation',\n",
    "                           line=dict(color='#636EFA', dash='dash'))\n",
    "\n",
    "    dash_obj2 = go.Scatter(x=df.index,\n",
    "                           y=df['Post Sanitation'],\n",
    "                           name='Post Sanitation',\n",
    "                           line=dict(color='#FF8800'))\n",
    "\n",
    "    fig.update_layout(xaxis_title='Datetime',\n",
    "                      yaxis_title=ylabel,\n",
    "                      template='plotly_white', autosize=False,\n",
    "                      width=1000, height=350,\n",
    "                      legend=dict(orientation=\"h\", yanchor=\"top\",\n",
    "                                  y=1.02, xanchor=\"right\", x=0.33))\n",
    "\n",
    "    fig.add_trace(dash_obj1)\n",
    "    fig.add_trace(dash_obj2)\n",
    "    fig.update_layout(margin=dict(l=20, r=20, t=5, b=20))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_to_uri(in_fig, close_all=True, **save_args):\n",
    "    \"\"\"\n",
    "    Save a figure as a URI\n",
    "    :param in_fig: Input figure.\n",
    "    :return: str.\n",
    "    \"\"\"\n",
    "    out_img = io.BytesIO()\n",
    "    in_fig.savefig(out_img, format = 'png', **save_args)\n",
    "    if close_all:\n",
    "        in_fig.clf()\n",
    "        plt.close('all')\n",
    "    out_img.seek(0)  # rewind file\n",
    "    encoded = base64.b64encode(out_img.read()).decode(\"ascii\").replace(\"\\n\", \"\")\n",
    "    return \"data:image/png;base64,{}\".format(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_card_content(card_header,data_type, overall_value, color):\n",
    "    card_head_style = {'textAlign':'center','fontSize':'100%', 'color': color}\n",
    "    card_body_style = {'textAlign':'center','fontSize':'300%', 'color':color, 'background-color':'whitesmoke'} \n",
    "    card_header = dbc.CardHeader(card_header,style=card_head_style)\n",
    "    card_body = dbc.CardBody(\n",
    "        [\n",
    "            html.H5(\"{}\".format(data_type), className=\"card-text\",\n",
    "                                        style={'textAlign':'center',\"font-weight\": \"bold\",'fontSize':'90%',\n",
    "                                              'color':color}),\n",
    "            html.H3(\"{}\".format(overall_value),\n",
    "                className=\"card-title\",style={'textAlign':'center',\"font-weight\": \"bold\",'fontSize':'200%', 'color':color}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    card = [card_header,card_body]\n",
    "    return card\n",
    "\n",
    "def generate_cards1(data_summary):\n",
    "    \n",
    "    total_data = data_summary.loc['Data Points Available']['Values']\n",
    "    missing = str(data_summary.loc['Missing Data (%)']['Values']) + '%'\n",
    "    outlier = str(data_summary.loc['Outliers (%)']['Values']) + '%'\n",
    "    time_reso = data_summary.loc['Temporal Resolution']['Values']\n",
    "    missing_data = int(data_summary.loc['Missing Data (%)']['Values'])\n",
    "    outlier_data = int(data_summary.loc['Outliers (%)']['Values'])\n",
    "    print('###########Inside generate cards 1 - printing #######')\n",
    "    print(total_data, 'total_data')\n",
    "    print('only str missing', missing)\n",
    "    print('only str outlier', outlier)\n",
    "    print('outlier data:', outlier_data)\n",
    "    print('missing_data:', missing_data)\n",
    "    \n",
    "    if ((100-(missing_data+outlier_data)) > 99.5):\n",
    "        status =\"Superb\"\n",
    "    elif ((100-(missing_data+outlier_data)) > 98) & ((100-(missing_data+outlier_data)) < 99.5):\n",
    "        status= \"Good\"\n",
    "    elif ((100-(missing_data+outlier_data)) > 95) & ((100-(missing_data+outlier_data)) < 98):\n",
    "        status = \"Normal\"\n",
    "    else:\n",
    "        status='Poor'\n",
    "    print('####status', status)\n",
    "\n",
    "    cards = html.Div(\n",
    "        [\n",
    "#             html.H4(\"Data Quality: Pre Data sanitation\",\n",
    "#             style={'textAlign': 'center',\n",
    "#                    'fontColor':'#333333'}),\n",
    "            \n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dbc.Card(generate_card_content(card_header=\"Pre-Sanitation\",data_type='Total',\n",
    "                                                           overall_value=total_data, color='#1F77B4'), \n",
    "                                     inverse=True),md=dict(size=2,offset=2),width=\"auto\"),\n",
    "                    dbc.Col(dbc.Card(generate_card_content(card_header=\"Pre-Sanitation\",data_type='Missing',\n",
    "                                                           overall_value=missing,color='#1F77B4'), \n",
    "                                     inverse=True),md=dict(size=2),width=\"auto\"),\n",
    "                    dbc.Col(dbc.Card(generate_card_content(card_header=\"Pre-Saniattion\",data_type='outlier',\n",
    "                                                           overall_value=outlier,color='#1F77B4'), \n",
    "                                     inverse=True),md=dict(size=2),width=\"auto\"),\n",
    "                    dbc.Col(dbc.Card(generate_card_content(card_header=\"Pre-Sanitation\",data_type='Status',\n",
    "                                                           overall_value=status,color='#1F77B4'), \n",
    "                                     inverse=True),md=dict(size=2),width=\"auto\"),\n",
    "                ],\n",
    "                className=\"mb-6\",\n",
    "            ),\n",
    "        ],id='card1'\n",
    "    )\n",
    "    return cards\n",
    "\n",
    "def generate_cards2(data_summary):\n",
    "    \n",
    "    missing_data_post_sanitation = int(data_summary.loc['missing_data_post_sanitation']['Values'])\n",
    "    missing = str(data_summary.loc['missing_data_post_sanitation']) + '%'\n",
    "    total_data = total_data = data_summary.loc['Data Points Available']['Values']\n",
    "    outlier ='0 %'\n",
    "    print('Inside generate cards 2 - printing #######')\n",
    "    print(total_data, 'total_data')\n",
    "    print('missing_data:', missing)\n",
    "    print('outlier data:', outlier)\n",
    "\n",
    "    if ((100-(missing_data_post_sanitation)) > 99.5):\n",
    "        status =\"Superb\"\n",
    "    elif ((100-(missing_data_post_sanitation)) > 98) & ((100-(missing_data_post_sanitation)) < 99.5):\n",
    "        status= \"Good\"\n",
    "    elif ((100-(missing_data_post_sanitation)) > 95) & ((100-(missing_data_post_sanitation)) < 98):\n",
    "        status = \"Normal\"\n",
    "    else:\n",
    "        status='Poor'\n",
    "    print('####status', status)\n",
    "        \n",
    "    cards = html.Div(\n",
    "        [\n",
    "#             html.H4(\"Data Quality: Post Data sanitation\",\n",
    "#         style={'textAlign': 'center',\n",
    "#                 'fontColor':'#333333'}),\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dbc.Card(generate_card_content(\"Post-Sanitation\",\"Datapoints\",total_data,'#FF8800'), inverse=True),md=dict(size=2,offset=2), width=\"auto\"), #\n",
    "                    dbc.Col(dbc.Card(generate_card_content(\"Post-Sanitation\",\"Missing\",missing, '#FF8800'), inverse=True), md=dict(size=2), width=\"auto\"), #\n",
    "                    dbc.Col(dbc.Card(generate_card_content(\"Post-Sanitation\",\"Outlier\",outlier, \"#FF8800\"), inverse=True), md=dict(size=2), width=\"auto\"),\n",
    "                    dbc.Col(dbc.Card(generate_card_content(\"Post-Sanitation\",\"Status\",status, \"#FF8800\"), inverse=True), md=dict(size=2), width=\"auto\"),\n",
    "                ],\n",
    "                className=\"mb-6\",\n",
    "            ),\n",
    "        ],id='card2'\n",
    "    )\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_footer():\n",
    "    return html.Footer(\n",
    "        children=[\"©smarthelio2022\"],\n",
    "        style={'width':'100%', 'color': '#FFFFFF',\n",
    "               'text-align': 'center', 'position': 'absolute', \n",
    "               'background-color': '#737373', 'left': 0,\n",
    "               'bottom': 0, 'font-family': 'Roboto'}\n",
    "    )\n",
    "\n",
    "from dash import html\n",
    "import base64\n",
    "image_filename = 'assets/SmartHelio logo-2 (1).png' # replace with your own image\n",
    "encoded_image = base64.b64encode(open(image_filename, 'rb').read())\n",
    "\n",
    "\n",
    "search_bar1 = dbc.Row(\n",
    "    [\n",
    "        dbc.Col(dbc.NavItem(dbc.NavLink(\"Home\", href=\"/home\",\n",
    "                                        style={\"color\":\"#FFFFFF\", \"font-family\": \"Roboto\", 'fontSize':'110%',\n",
    "                                              'justify':'right'})),\n",
    "                width=\"auto\"),\n",
    "        dbc.Col(dbc.NavItem(dbc.NavLink(\"Data Sanitation\", href=\"/data_sanitation_dashboard\",\n",
    "                                        style={\"color\":\"#FFFFFF\", \"font-family\": \"Roboto\", 'fontSize':'110%',\n",
    "                                              'justify':'right'})),\n",
    "                width=\"auto\"),\n",
    "        dbc.Col(dbc.NavItem(dbc.NavLink(\"API Connect\", href=\"#\",\n",
    "                                        style={\"color\":\"#FFFFFF\", \"font-family\": \"Roboto\",'fontSize':'110%',\n",
    "                                              'justify':'right'})),\n",
    "                width=\"auto\"),\n",
    "        \n",
    "        dbc.Col(dbc.NavItem(dbc.NavLink(\"Learn More\", href=\"/learn_more\",\n",
    "                                        style={\"color\":\"#FFFFFF\", \"font-family\": \"Roboto\",'fontSize':'110%',\n",
    "                                              'justify':'right'})),\n",
    "                width=\"auto\"),\n",
    "        \n",
    "    ],\n",
    "    className=\"g-0 ms-auto flex-nowrap mt-0 mt-md-3\",\n",
    "    align=\"left\",\n",
    ")\n",
    "\n",
    "def Navbar():\n",
    "    return dbc.Navbar(\n",
    "        dbc.Container(\n",
    "        [\n",
    "            html.A(\n",
    "                # Use row and col to control vertical alignment of logo / brand\n",
    "                dbc.Row(\n",
    "                    [\n",
    "                        dbc.Col(html.Img(\n",
    "                                src='data:image/png;base64,{}'.format(encoded_image.decode()),\n",
    "                                height=\"25px\")),\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                    className=\"g-0\",\n",
    "                ),\n",
    "                href=\"https://smarthelio.com\",\n",
    "                style={\"textDecoration\": \"none\"},\n",
    "            ),\n",
    "            search_bar1 \n",
    "        ]\n",
    "    ),\n",
    "    color=\"#737373\",\n",
    "    dark=True,\n",
    "    sticky='top',\n",
    "#     fluid=True,\n",
    ")\n",
    "\n",
    "\n",
    "body = dbc.Container(\n",
    "    [\n",
    "        html.Br(),\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(html.H2(\"Welcome to SmartHelio's Data Sanitation Dashboard\", style={'font-family':'Roboto'}),\n",
    "                        width={\"size\":10,\"offset\":1.5}, style={'color':'#333333'}),\n",
    "            ],\n",
    "            justify=\"center\",\n",
    "        ),\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(dcc.Markdown(\n",
    "                    \"\"\"\n",
    "                    Data Sanitation dashboard is the only application that helps you get preprocessed and sanitized data of your solar plant \n",
    "                    using SmartHelio's proprietary AI based Data Sanitation Algorithm in seconds.\"\n",
    "                    \"\"\",\n",
    "                    style={'font-family':'Roboto','text-align':'center'}\n",
    "                ),\n",
    "                width={\"size\":8, \"offset\":0.6}, \n",
    "                style={'color':'#333333'}\n",
    "                       ),\n",
    "            ],\n",
    "            justify=\"center\",\n",
    "        ),\n",
    "        html.Br(),\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(\n",
    "                    [\n",
    "                        html.Img(\n",
    "                            src='/assets/solar-panel-health-web.jpg', height=\"360px\",\n",
    "                            style={'padding-left': '200px'}\n",
    "                        ), \n",
    "                    ],\n",
    "                    md=5\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        html.Br(),\n",
    "        html.Br(),\n",
    "        html.Div(\n",
    "            [\n",
    "                dbc.Button(\"Contact Us\", color=\"dark\",\n",
    "                           outline=True, href=\"https://smarthelio.com\",\n",
    "                           external_link=True, style={'font-family': 'Roboto'}\n",
    "                          ),\n",
    "            ],\n",
    "            className=\"d-grid gap-0 col-1 mx-auto\",\n",
    "        ),\n",
    "    ],\n",
    "    className=\"mt-4\",\n",
    ")\n",
    "\n",
    "def Homepage():\n",
    "    layout = html.Div([\n",
    "        Navbar(),\n",
    "        body\n",
    "    ])\n",
    "    return layout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPLOAD BUTTON\n",
    "def upload_data_card():\n",
    "    \"\"\"\n",
    "    :return: A Div containing an Upload button\n",
    "    \"\"\"\n",
    "    return html.Div(\n",
    "        id='upload-data-card',\n",
    "        children=[\n",
    "            dcc.Upload(\n",
    "                id='upload-data',\n",
    "                children=html.Div([html.Button('Upload or Drag Files', style={'backgroundColor': 'whitesmoke', \n",
    "                                                                              'Color': '#333333'})]),\n",
    "                style={'width': '150%', 'height': '30px',\n",
    "                       'textAlign': 'center', 'font-family':'Roboto',\n",
    "                      'border-radius': '8px','padding-right':'280px'},\n",
    "                # Don't allow multiple files to be uploaded\n",
    "                multiple=False\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# LEFT SIDE TAB INFORMATION\n",
    "def description_card():\n",
    "    \"\"\"\n",
    "    :return: A Div containing dashboard title & descriptions.\n",
    "    \"\"\"\n",
    "    return html.Div(\n",
    "        id=\"description-card\",\n",
    "        children=[\n",
    "        html.H5(\"SmartHelio\", style={'color':'#FF8800',\n",
    "                                     \"font-weight\": \"bold\",\n",
    "                                    'font-family': 'Roboto',\n",
    "                                    'fontSize':'150%'}),\n",
    "            html.H3(\"Welcome to the Data Sanitation Dashboard\", \n",
    "                    style={'fontColor':'#FF8800',\n",
    "                           'font-family': 'Roboto'}),\n",
    "            html.H6(\"Get your data sanitized in one click.\", \n",
    "                    style={'font-family': 'Roboto'}),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# LOGO ON THE DASHBOARD\n",
    "def generate_control_card():\n",
    "    \"\"\"\n",
    "    :return: A Div containing controls for graphs.\n",
    "    \"\"\"\n",
    "    return html.Div(\n",
    "        id=\"control-card\",\n",
    "        children=[\n",
    "            html.P(\"\"),\n",
    "            html.Div(\n",
    "                id=\"banner-logo\",\n",
    "                children=[\n",
    "                    upload_data_card()]),\n",
    "            html.Br(),\n",
    "            html.Div(\n",
    "                id=\"reset-btn-outer\",\n",
    "                children=html.Button(id=\"apply-btn\", children=\"Apply\",\n",
    "                                     n_clicks=0),\n",
    "            ),\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_modal():\n",
    "    return html.Div(\n",
    "        id=\"markdown\",\n",
    "        className=\"modal\",\n",
    "        children=(\n",
    "            html.Div(\n",
    "                id=\"markdown-container\",\n",
    "                className=\"markdown-container\",\n",
    "                children=[\n",
    "                    html.Div(\n",
    "                        className=\"close-container\",\n",
    "                        children=html.Button(\n",
    "                            \"Close\",\n",
    "                            id=\"markdown_close\",\n",
    "                            n_clicks=0,\n",
    "                            className=\"closeButton\",\n",
    "                        ),\n",
    "                    ),\n",
    "                    html.Div(\n",
    "                        className=\"markdown-text\",\n",
    "                        children=dcc.Markdown(\n",
    "                            children=(\n",
    "                                \"\"\"\n",
    "                         ***What is this app about?***\n",
    "                         SmartHelio's Data Sanitation Dashboard is the only application that helps you preprocess\n",
    "                         and sanitize your solar plant's data in seconds using **SmartHelio's properietary AI based \n",
    "                         Data Preprocessing Algorithm**. \n",
    "                         \n",
    "                         ***How does Data sanitation tool work?***\n",
    "                         Simply upload your data you want to get preprocessed and download your clean and sanitized data in seconds!\n",
    "                            \n",
    "                        ***How much time does it take to preprocess the data?***\n",
    "                         Depending upon the size and quality of your data, it should take about a few minutes.\n",
    "\n",
    "                        ***Why should you choose SmartHelio's Automated Data Sanitation Tool?***\n",
    "                         Because we have industry experience and we work one of the the best research instituions in the world\n",
    "                         such EPFL, HSLU, etc to create our algorithms.\n",
    "                        \n",
    "                        **Glossary**\n",
    "                        ***Data Points:***\n",
    "                        The total measurements received in the dataset.\n",
    "                            \n",
    "                        ***Missing %:*** \n",
    "                        It refers to % of the datapoints given as 'NaN' i.e. not a number in the dataset. \n",
    "                            \n",
    "                        ***Outlier %:*** \n",
    "                        It refers to % of the datapoints given as very high or very low (or negative) values which degrades the \n",
    "                        quality of dataset. \n",
    "                            \n",
    "                        ***Status:*** \n",
    "                        It refers to the overall quality of the dataset after taking into the account missing, bad, \n",
    "                        and outlier values. \n",
    "                        \n",
    "                        ***Download Sanitized Data:*** \n",
    "                        To download the preprocessed and sanitized data in a csv, simply click the \"Download \n",
    "                        Sanitized Data\". \n",
    "                        \n",
    "                        ***Upload Data:*** \n",
    "                        This button helps to upload the data which needs to be cleaned and sanitized. \n",
    "                        \"\"\"\n",
    "                            ),\n",
    "                            style={'backgroundColor': '#333333','color': '#FFFFFF','font-family': 'Roboto'},\n",
    "                        ),\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_multiindex_dataframe(dataframe_json: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Deserialize the dataframe json into a dataframe object.\n",
    "    The dataframe json must be generated with DataFrame.to_json(orient=\"split\")\n",
    "    This function is to address the fact that `pd.read_json()` isn't behaving correctly (yet)\n",
    "    https://github.com/pandas-dev/pandas/issues/4889\n",
    "    \"\"\"\n",
    "    def convert_index(json_obj):\n",
    "        to_tuples = [tuple(i) if isinstance(i, list) else i for i in json_obj]\n",
    "        if all(isinstance(i, list) for i in json_obj):\n",
    "            return pd.MultiIndex.from_tuples(to_tuples)\n",
    "        else:\n",
    "            return pd.Index(to_tuples)\n",
    "    json_dict = json.loads(dataframe_json)\n",
    "    columns = convert_index(json_dict['columns'])\n",
    "    index = convert_index(json_dict['index'])\n",
    "    dataframe = pd.DataFrame(json_dict[\"data\"], index, columns)\n",
    "    return dataframe\n",
    "\n",
    "## Reading the uploaded file \n",
    "@app.callback(\n",
    "    Output('intermediate-value', 'data'),\n",
    "    [\n",
    "        Input('upload-data', 'contents'),\n",
    "        Input('upload-data', 'filename')\n",
    "    ],\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def create_data(contents, filename):\n",
    "    # Starting the timer\n",
    "    start_time = time.time()\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string)\n",
    "\n",
    "    try:\n",
    "        if 'csv' in filename:\n",
    "            array_info, general_info = gather_inputs(\n",
    "                io.StringIO(decoded.decode('utf-8')))\n",
    "\n",
    "            inverter_data,data_points = read_inverter_data(\n",
    "                general_info, io.StringIO(decoded.decode('utf-8')))\n",
    "            \n",
    "            meteo_data, irr_df = read_weather_data(\n",
    "                general_info, io.StringIO(decoded.decode('utf-8')))\n",
    "\n",
    "        elif 'xls' or 'xlsx' in filename:\n",
    "            array_info, general_info = gather_inputs(io.BytesIO(decoded))\n",
    "            print(array_info)\n",
    "                        \n",
    "            inverter_data,data_points = read_inverter_data(general_info, io.BytesIO(decoded))\n",
    "            print(inverter_data)\n",
    "            print('INVERTER DATA PROCESSED')\n",
    "            meteo_data,irr_df = read_weather_data(general_info, io.BytesIO(decoded))\n",
    "            print(meteo_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return html.Div(['There was an error processing this file.'])\n",
    "    \n",
    "    # converting irradinace GHI to POA \n",
    "    meteo_data = get_operational_irradiance(\n",
    "        meteo_data, general_info, array_info, poa_model='isotropic')\n",
    "    \n",
    "    # Data Sanitization- meteo\n",
    "    meteo_data_filtered = multiindex_irradiance_filter(meteo_data,\n",
    "                                                       irrad_low=0,\n",
    "                                                       irrad_high=1200)\n",
    "    # Clear sky curve\n",
    "    csky_curve = clearsky_irradiance(times=inverter_data.index, general_info=general_info,\n",
    "                                          array_info=array_info, convertGHI_toPOA=True)\n",
    "\n",
    "    tz_str = get_tz(latitude=general_info['lat'], longitude=general_info['long'])\n",
    "    csky_curve.index = csky_curve.index.tz_localize('UTC').tz_convert(tz_str).tz_localize(None)\n",
    "\n",
    "    inverter_data_csky = eliminate_nightvalues(inverter_data,\n",
    "                                               cs_data=csky_curve, threshold=10)\n",
    "    \n",
    "    # Checking for % of missing data\n",
    "    missing_data = round((inverter_data_csky.isna().sum().sum()/inverter_data_csky.size)*100,2)\n",
    "    print('Missing data for Inverter is {}'.format(missing_data))\n",
    "    \n",
    "    # Data Sanitization-inverter\n",
    "    inverter_data_filtered = multiindex_current_filter(inverter_data_csky, array_info)\n",
    "    inverter_data_filtered = multiindex_voltage_filter(inverter_data_filtered,\n",
    "                                                       array_info)\n",
    "    # % of outliers\n",
    "    outlier_data = round(((inverter_data_filtered.isna().sum().sum()/\n",
    "                               inverter_data_csky.isna().sum().sum())),2)\n",
    "    print('Outliers: ', outlier_data)\n",
    "\n",
    "    # meteo data - for graph\n",
    "    csky_curve_meteo = clearsky_irradiance(times=irr_df.index, general_info=general_info,\n",
    "                                          array_info=array_info, convertGHI_toPOA=True)\n",
    "    csky_curve_meteo.index = csky_curve_meteo.index.tz_localize('UTC').tz_convert(tz_str).tz_localize(None)\n",
    "    meteo_data_csky = eliminate_nightvalues(meteo_data_filtered,\n",
    "                                               cs_data=csky_curve_meteo, threshold=10)\n",
    "    \n",
    "        # Timer ends here\n",
    "    end_time = time.time()\n",
    "    print('File Execution Time is {} seconds'.format(end_time - start_time))\n",
    "\n",
    "    if missing_data > 0.5:\n",
    "        print('\\n MISSING DATA FOUND!!')\n",
    "        print('\\n Computing Missing Data using Machine Learning Models')\n",
    "        inverter_data_sanitized = predict_missing_data(inverter_data_filtered,\n",
    "                                                       meteo_data_filtered,\n",
    "                                                       array_info, general_info)\n",
    "        # converting df into a multi-index df\n",
    "        colname = [(i, j, z) for i, j, z in [x.split('-') for x in inverter_data_sanitized.columns]]\n",
    "        inverter_data_sanitized.columns = pd.MultiIndex.from_tuples(colname, names=['ag_level_2', 'ag_level_1', 'curve'])\n",
    "        inverter_data_sanitized.index.names = ['datetime']\n",
    "\n",
    "    else:\n",
    "        inverter_data_sanitized = inverter_data_csky.fillna(method = 'ffill').fillna(method='bfill')\n",
    "        print('\\nData Availability {} %'.format(100 - missing_data))\n",
    "        print('\\n FINAL STATUS : GOOD FOR ANALYSIS')\n",
    "\n",
    "    inverter_data_sanitized[inverter_data_sanitized<0] = np.nan\n",
    "    missing_data_post_sanitation = round((inverter_data_sanitized.isna().sum().sum()/inverter_data_csky.size)*100,2)\n",
    "    \n",
    "    data_summary = pd.DataFrame(index=['Data Points Available',\n",
    "                                         'Temporal Resolution', 'Missing Data (%)',\n",
    "                                         'Outliers (%)', 'missing_data_post_sanitation'], columns=['Values'])\n",
    "\n",
    "    data_summary.loc['Data Points Available'] = str(data_points/1000) + ' K'\n",
    "    data_summary.loc['Temporal Resolution'] = str(general_info['inverter_time_resolution']) + ' Mins'\n",
    "    data_summary.loc['Missing Data (%)'] = missing_data\n",
    "    data_summary.loc['Outliers (%)'] = outlier_data\n",
    "    data_summary.loc['missing_data_post_sanitation'] = missing_data_post_sanitation\n",
    "    data_summary = data_summary.replace(np.nan,0)\n",
    "    print('Printing data summary in reading files:', data_summary)\n",
    "    print('#####################')\n",
    "\n",
    "    print(array_info)\n",
    "#     converting dataframes into json object\n",
    "    datasets = {\n",
    "        'array_info': array_info.to_json(orient='split', date_format='iso'),\n",
    "        'inv_data': inverter_data.to_json(orient='split', date_format='iso'),\n",
    "        'inv_data_csky': inverter_data_csky.to_json(orient='split', date_format='iso'),\n",
    "        'inv_data_sani': inverter_data_sanitized.to_json(orient='split', date_format='iso'),\n",
    "        'meteo_data': meteo_data.to_json(orient='split', date_format='iso'),\n",
    "        'irr_df': irr_df.to_json(orient='split', date_format='iso'),\n",
    "        'meteo_data_csky': meteo_data_csky.to_json(orient='split', date_format='iso'),\n",
    "        'data_summary': data_summary.to_json(orient='index'),\n",
    "        'general_info': json.dumps(general_info)\n",
    "    } \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Timt taken for processing the data: {}'.format(end_time-start_time))\n",
    "    \n",
    "    return json.dumps(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASHBOARD APP LAYOUT\n",
    "\n",
    "app.layout = html.Div(\n",
    "    id=\"app-container\",\n",
    "    children=[\n",
    "        Navbar(),\n",
    "#             # Banner\n",
    "#             html.Div(\n",
    "#                 id=\"banner\",\n",
    "#                 className=\"banner\",\n",
    "#                 children=[\n",
    "#                     html.A(\n",
    "#                         html.Img(src=app.get_asset_url(\"SH_logo.png\")),\n",
    "#                         href=\"https://smarthelio.com/\",\n",
    "#                     ),\n",
    "#                     html.A(      \n",
    "#                         html.Button(\n",
    "#                             id=\"learn-more-button\", children=\"Learn more\",\n",
    "#                             n_clicks=0,\n",
    "#                             style={'background-color': '#D9D9D9','color': '#333333',\n",
    "#                                    'border-color': '#333333', 'width':'200px',\n",
    "#                                   'font-family': 'Roboto',  'border-radius': '12px'},\n",
    "#                         ),\n",
    "#                     )\n",
    "#                 ],\n",
    "#             ),\n",
    "\n",
    "                # Left column\n",
    "                html.Div(\n",
    "                    id=\"left-column\",\n",
    "                    className=\"four columns\",\n",
    "                    children=[\n",
    "                        description_card(),\n",
    "                        html.Br(),\n",
    "                        upload_data_card(),\n",
    "                        html.Br(),\n",
    "                #                 html.H4('----OR----',style={'font-family': 'Roboto',\n",
    "#                                        'fontSize':'130%', 'font-weight':'bold'}),\n",
    "\n",
    "#                 html.Br(),\n",
    "#                 html.Label('Select a demo plant',style={'font-family':'Roboto', 'font-weight':'bold'}),\n",
    "#                 dcc.Dropdown(\n",
    "#                             id='plant-select',\n",
    "#                             options=[\n",
    "#                                 {'label': 'Delhi, India', 'value': 'AB'},\n",
    "#                                 {'label': 'Lausanne, Switzerland', 'value': 'RE'}\n",
    "#                             ],\n",
    "#                       style = {'textAlign':'center',\n",
    "#                                'width':'350px',\n",
    "#                                'height':'40px',\n",
    "#                               'border-radius': '8px',\n",
    "#                               'background-color': 'whitesmoke'}\n",
    "#                           ),\n",
    "                    html.Br(),\n",
    "                    html.Br(),\n",
    "#                 html.Button(\"Submit\", id=\"submit-btn\",\n",
    "#                             style={'width':'120px',\n",
    "#                                    'height':'50px',\n",
    "#                                    'border-radius': '8px',\n",
    "#                                    'fontSize':'90%',\n",
    "#                                       }),\n",
    "                    html.Br(),\n",
    "                    html.Br(),\n",
    "                    html.Button(\"Download Sanitized Data\", id=\"btn-download-txt\",\n",
    "                                style={'background-color': '#737373','color': '#FFFFFF', 'width':'280px',\n",
    "                                       'border-radius': '8px',\n",
    "                                        'fontSize':'85%'}),\n",
    "                    dcc.Download(id='download_data'),\n",
    "                ],\n",
    "            ),\n",
    "                #Right column\n",
    "                dcc.Tabs(\n",
    "                    id=\"stitching-tabs\",\n",
    "                    value=\"data-summary-tab\",\n",
    "                    style={'height': '5%'},\n",
    "                    children=[\n",
    "                        dcc.Tab(\n",
    "                            label=\"Plant Level Data Summary\",\n",
    "                            value=\"data-summary-tab\",\n",
    "                            selected_style=tab_style,\n",
    "                            children=[\n",
    "            #                             html.H4(\"Plant Level Data Summary\",\n",
    "            #                                     style={'textAlign': 'center',\n",
    "            #                                            'fontColor':'#333333'}),\n",
    "                                html.Br(),\n",
    "                                html.Div(id='data-values-pre'),\n",
    "                                html.Br(),\n",
    "                                html.Div(id='data-values-post'),\n",
    "\n",
    "#                             html.H4(\"Input Level Data Summary\",\n",
    "#                                      style={'textAlign': 'center',\n",
    "#                                             'fontColor':'#333333'}),\n",
    "#                             html.Div(id='input_level_smmary',\n",
    "#                                      children=[\n",
    "#                                          dcc.Loading(id=\"loading-1\",\n",
    "#                                                      children=[dcc.Graph(id='bar_plot_missing',\n",
    "#                                                                          config= {'displaylogo': False})],\n",
    "#                                                      type=\"circle\"),]),\n",
    "                    html.Br(),\n",
    "                    dbc.Row([\n",
    "                        dbc.Col(id='card-markdown'),\n",
    "                    ],\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            dcc.Tab(\n",
    "                label=\"Input Level Data Summary\",\n",
    "                value=\"weather-data-tab\",\n",
    "                selected_style=tab_style,\n",
    "                children=[\n",
    "                    html.Br(),\n",
    "                    html.P(\"The graph below shows input level pre-data sanitation comparison in terms of data quality i.e. total data points available for anlaysis, missing data points (%), outliers found (%), etc.\",\n",
    "                          style={'font-family': 'Roboto'}),\n",
    "                    dcc.Loading(id=\"loading-1\",\n",
    "                                children=[dcc.Graph(id='bar_plot_missing',config= {'displaylogo': False})],\n",
    "                                type=\"circle\"),\n",
    "                ],\n",
    "             ),\n",
    "            dcc.Tab(\n",
    "                label=\"Data Visualization\",\n",
    "                value=\"sanitized-data-tab\",\n",
    "                selected_style=tab_style,\n",
    "                children=[\n",
    "                    html.Br(),\n",
    "                    html.P(\"The graphs below helps you visualize how the data has been cleaned and processed. The dark grey curve represents raw data and light grey curve represents processed data post sanitation. The overlap of dark grey and light grey curve represents good data points in the dataset.\",\n",
    "                          style={'font-family': 'Roboto'}),\n",
    "                    html.Br(),\n",
    "                    html.Div(\n",
    "                        id=\"sanitized_data_tab\",\n",
    "                        children=[\n",
    "                            html.B(\"Select Input for Analysis\"),\n",
    "                            html.Div(dcc.Dropdown(id='input-select', persistence=True), id='input-container'),\n",
    "                            html.Br(),\n",
    "                            dcc.Loading(\n",
    "                                id=\"loading-2\",\n",
    "                                children=[dcc.Graph(id='current_graph',config= {'displaylogo': False})],\n",
    "                                type=\"circle\"\n",
    "                            ),\n",
    "                            dcc.Loading(\n",
    "                                id=\"loading-3\",\n",
    "                                children=[dcc.Graph(id='voltage_graph',config= {'displaylogo': False})],\n",
    "                                type=\"circle\"\n",
    "                            ),\n",
    "\n",
    "                            dcc.Loading(\n",
    "                                id=\"loading-4\",\n",
    "                                children=[dcc.Graph(id='irradiance_graph',config= {'displaylogo': False})],\n",
    "                                type=\"circle\"\n",
    "                            ),\n",
    "                            html.Br(),\n",
    "                            html.Br(),\n",
    "                        ]\n",
    "                    )]\n",
    "            ),\n",
    "\n",
    "        ],\n",
    "    ),            \n",
    "    generate_modal(),\n",
    "    dcc.Store(id='intermediate-value', storage_type = 'session')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8080/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [18/Feb/2022 11:33:59] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"GET /_dash-component-suites/dash/dcc/async-upload.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"GET /_dash-component-suites/dash/dcc/async-markdown.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 99, in update_data_summary\n",
      "    datasets = json.loads(jsonified_cleaned_data)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\json\\__init__.py\", line 341, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"POST /_dash-update-component HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 111, in update_data_summary\n",
      "    datasets = json.loads(jsonified_cleaned_data)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\json\\__init__.py\", line 341, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"POST /_dash-update-component HTTP/1.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 64, in bar_plot_graph\n",
      "    datasets = json.loads(jsonified_cleaned_data)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\json\\__init__.py\", line 341, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\" 500 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"POST /_dash-update-component HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 130, in update_current_graph\n",
      "    datasets = json.loads(jsonified_cleaned_data)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\json\\__init__.py\", line 341, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"POST /_dash-update-component HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 7, in update_output\n",
      "    datasets = json.loads(jsonified_cleaned_data)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\json\\__init__.py\", line 341, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 181, in func2\n",
      "    return dcc.send_data_frame(inverter_data_sanitized.to_csv,\n",
      "UnboundLocalError: local variable 'inverter_data_sanitized' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:00] \"GET /_dash-component-suites/dash/dcc/async-highlight.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected system: Oiken\n",
      "                      inverter_ID input_name  sensor_ID  installed_capacity  \\\n",
      "ag_level_2 ag_level_1                                                         \n",
      "Inv1       M1             Inv1-M1    Inv1-M1        NaN           11790.868   \n",
      "           M2             Inv1-M2    Inv1-M2        NaN           11790.868   \n",
      "Inv2       M1             Inv2-M1    Inv2-M1        NaN           11790.868   \n",
      "           M2             Inv2-M2    Inv2-M2        NaN           11170.296   \n",
      "Inv3       M1             Inv3-M1    Inv3-M1        NaN           11790.868   \n",
      "           M2             Inv3-M2    Inv3-M2        NaN           11170.296   \n",
      "\n",
      "                       number_of_modules  number_of_strings  \\\n",
      "ag_level_2 ag_level_1                                         \n",
      "Inv1       M1                         38                  2   \n",
      "           M2                         38                  2   \n",
      "Inv2       M1                         38                  2   \n",
      "           M2                         36                  2   \n",
      "Inv3       M1                         38                  2   \n",
      "           M2                         36                  2   \n",
      "\n",
      "                       modules_per_string  i_mpp  v_mpp  i_sc  ...  \\\n",
      "ag_level_2 ag_level_1                                          ...   \n",
      "Inv1       M1                          19   9.29   33.4  9.77  ...   \n",
      "           M2                          19   9.29   33.4  9.77  ...   \n",
      "Inv2       M1                          19   9.29   33.4  9.77  ...   \n",
      "           M2                          18   9.29   33.4  9.77  ...   \n",
      "Inv3       M1                          19   9.29   33.4  9.77  ...   \n",
      "           M2                          18   9.29   33.4  9.77  ...   \n",
      "\n",
      "                       nominal_efficiency  module_area  gamma    beta   alpha  \\\n",
      "ag_level_2 ag_level_1                                                           \n",
      "Inv1       M1                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "           M2                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "Inv2       M1                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "           M2                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "Inv3       M1                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "           M2                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "\n",
      "                       surface_tilt  surface_azimuth  inverter_eff  \\\n",
      "ag_level_2 ag_level_1                                                \n",
      "Inv1       M1                     6              244         0.982   \n",
      "           M2                     6              244         0.982   \n",
      "Inv2       M1                     6              244         0.982   \n",
      "           M2                     6              244         0.982   \n",
      "Inv3       M1                     6              244         0.982   \n",
      "           M2                     6              244         0.982   \n",
      "\n",
      "                       pollution_red_factor  expected_degradation  \n",
      "ag_level_2 ag_level_1                                              \n",
      "Inv1       M1                             1                 2.612  \n",
      "           M2                             1                 2.612  \n",
      "Inv2       M1                             1                 2.612  \n",
      "           M2                             1                 2.612  \n",
      "Inv3       M1                             1                 2.612  \n",
      "           M2                             1                 2.612  \n",
      "\n",
      "[6 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 11:34:06 : Check timestamp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data using Pecos\n",
      "percos cleaning complete\n",
      "array_info_read\n",
      "inverter_data_now_complete\n",
      "ag_level_2          Inv1                  Inv2                  Inv3        \\\n",
      "ag_level_1            M1         M2         M1         M2         M1         \n",
      "curve                  I  P  V    I  P  V    I  P  V    I  P  V    I  P  V   \n",
      "datetime                                                                     \n",
      "2020-04-01 00:00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-04-01 00:10:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-04-01 00:20:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-04-01 00:30:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-04-01 00:40:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "...                  ... .. ..  ... .. ..  ... .. ..  ... .. ..  ... .. ..   \n",
      "2020-04-30 23:20:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-04-30 23:30:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-04-30 23:40:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-04-30 23:50:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "2020-05-01 00:00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0   \n",
      "\n",
      "ag_level_2                      \n",
      "ag_level_1            M2        \n",
      "curve                  I  P  V  \n",
      "datetime                        \n",
      "2020-04-01 00:00:00  0.0  0  0  \n",
      "2020-04-01 00:10:00  0.0  0  0  \n",
      "2020-04-01 00:20:00  0.0  0  0  \n",
      "2020-04-01 00:30:00  0.0  0  0  \n",
      "2020-04-01 00:40:00  0.0  0  0  \n",
      "...                  ... .. ..  \n",
      "2020-04-30 23:20:00  0.0  0  0  \n",
      "2020-04-30 23:30:00  0.0  0  0  \n",
      "2020-04-30 23:40:00  0.0  0  0  \n",
      "2020-04-30 23:50:00  0.0  0  0  \n",
      "2020-05-01 00:00:00  0.0  0  0  \n",
      "\n",
      "[4321 rows x 18 columns]\n",
      "INVERTER DATA PROCESSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 11:34:07 : Check timestamp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meteo file read\n",
      "array_file_read\n",
      "Cleaned Data using Pecos\n",
      "cleaned_using_pecos\n",
      "0.0 % of Irradiance Data is missing for analysis!\n",
      "0.0 % of Module Temp data is missing.\n",
      "ag_level_2          Inv1                          Inv2                      \\\n",
      "ag_level_1            M1             M2             M1             M2        \n",
      "curve                  G Tamb Tmod    G Tamb Tmod    G Tamb Tmod    G Tamb   \n",
      "datetime                                                                     \n",
      "2020-04-01 00:00:00  0.0  4.3  4.3  0.0  4.3  4.3  0.0  4.3  4.3  0.0  4.3   \n",
      "2020-04-01 00:10:00  0.0  3.3  3.3  0.0  3.3  3.3  0.0  3.3  3.3  0.0  3.3   \n",
      "2020-04-01 00:20:00  0.0  2.8  2.8  0.0  2.8  2.8  0.0  2.8  2.8  0.0  2.8   \n",
      "2020-04-01 00:30:00  0.0  2.9  2.9  0.0  2.9  2.9  0.0  2.9  2.9  0.0  2.9   \n",
      "2020-04-01 00:40:00  0.0  3.2  3.2  0.0  3.2  3.2  0.0  3.2  3.2  0.0  3.2   \n",
      "...                  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2020-04-30 23:10:00  0.0  7.9  7.9  0.0  7.9  7.9  0.0  7.9  7.9  0.0  7.9   \n",
      "2020-04-30 23:20:00  0.0  7.9  7.9  0.0  7.9  7.9  0.0  7.9  7.9  0.0  7.9   \n",
      "2020-04-30 23:30:00  0.0  8.2  8.2  0.0  8.2  8.2  0.0  8.2  8.2  0.0  8.2   \n",
      "2020-04-30 23:40:00  0.0  7.7  7.7  0.0  7.7  7.7  0.0  7.7  7.7  0.0  7.7   \n",
      "2020-04-30 23:50:00  0.0  7.8  7.8  0.0  7.8  7.8  0.0  7.8  7.8  0.0  7.8   \n",
      "\n",
      "ag_level_2               Inv3                           \n",
      "ag_level_1                 M1             M2            \n",
      "curve               Tmod    G Tamb Tmod    G Tamb Tmod  \n",
      "datetime                                                \n",
      "2020-04-01 00:00:00  4.3  0.0  4.3  4.3  0.0  4.3  4.3  \n",
      "2020-04-01 00:10:00  3.3  0.0  3.3  3.3  0.0  3.3  3.3  \n",
      "2020-04-01 00:20:00  2.8  0.0  2.8  2.8  0.0  2.8  2.8  \n",
      "2020-04-01 00:30:00  2.9  0.0  2.9  2.9  0.0  2.9  2.9  \n",
      "2020-04-01 00:40:00  3.2  0.0  3.2  3.2  0.0  3.2  3.2  \n",
      "...                  ...  ...  ...  ...  ...  ...  ...  \n",
      "2020-04-30 23:10:00  7.9  0.0  7.9  7.9  0.0  7.9  7.9  \n",
      "2020-04-30 23:20:00  7.9  0.0  7.9  7.9  0.0  7.9  7.9  \n",
      "2020-04-30 23:30:00  8.2  0.0  8.2  8.2  0.0  8.2  8.2  \n",
      "2020-04-30 23:40:00  7.7  0.0  7.7  7.7  0.0  7.7  7.7  \n",
      "2020-04-30 23:50:00  7.8  0.0  7.8  7.8  0.0  7.8  7.8  \n",
      "\n",
      "[4320 rows x 18 columns]\n",
      "Measured irradiance is already at POA\n",
      "Converting Clearsky GHI to Cleay sky POA...\n",
      "Missing data for Inverter is 0.0\n",
      "Outliers:  nan\n",
      "Converting Clearsky GHI to Cleay sky POA...\n",
      "File Execution Time is 10.988382577896118 seconds\n",
      "\n",
      "Data Availability 100.0 %\n",
      "\n",
      " FINAL STATUS : GOOD FOR ANALYSIS\n",
      "Printing data summary in reading files:                                 Values\n",
      "Data Points Available         155.52 K\n",
      "Temporal Resolution            10 Mins\n",
      "Missing Data (%)                   0.0\n",
      "Outliers (%)                         0\n",
      "missing_data_post_sanitation       0.0\n",
      "#####################\n",
      "                      inverter_ID input_name  sensor_ID  installed_capacity  \\\n",
      "ag_level_2 ag_level_1                                                         \n",
      "Inv1       M1             Inv1-M1    Inv1-M1        NaN           11790.868   \n",
      "           M2             Inv1-M2    Inv1-M2        NaN           11790.868   \n",
      "Inv2       M1             Inv2-M1    Inv2-M1        NaN           11790.868   \n",
      "           M2             Inv2-M2    Inv2-M2        NaN           11170.296   \n",
      "Inv3       M1             Inv3-M1    Inv3-M1        NaN           11790.868   \n",
      "           M2             Inv3-M2    Inv3-M2        NaN           11170.296   \n",
      "\n",
      "                       number_of_modules  number_of_strings  \\\n",
      "ag_level_2 ag_level_1                                         \n",
      "Inv1       M1                         38                  2   \n",
      "           M2                         38                  2   \n",
      "Inv2       M1                         38                  2   \n",
      "           M2                         36                  2   \n",
      "Inv3       M1                         38                  2   \n",
      "           M2                         36                  2   \n",
      "\n",
      "                       modules_per_string  i_mpp  v_mpp  i_sc  ...  \\\n",
      "ag_level_2 ag_level_1                                          ...   \n",
      "Inv1       M1                          19   9.29   33.4  9.77  ...   \n",
      "           M2                          19   9.29   33.4  9.77  ...   \n",
      "Inv2       M1                          19   9.29   33.4  9.77  ...   \n",
      "           M2                          18   9.29   33.4  9.77  ...   \n",
      "Inv3       M1                          19   9.29   33.4  9.77  ...   \n",
      "           M2                          18   9.29   33.4  9.77  ...   \n",
      "\n",
      "                       nominal_efficiency  module_area  gamma    beta   alpha  \\\n",
      "ag_level_2 ag_level_1                                                           \n",
      "Inv1       M1                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "           M2                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "Inv2       M1                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "           M2                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "Inv3       M1                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "           M2                       0.189        1.636 -0.004 -0.0034  0.0006   \n",
      "\n",
      "                       surface_tilt  surface_azimuth  inverter_eff  \\\n",
      "ag_level_2 ag_level_1                                                \n",
      "Inv1       M1                     6              244         0.982   \n",
      "           M2                     6              244         0.982   \n",
      "Inv2       M1                     6              244         0.982   \n",
      "           M2                     6              244         0.982   \n",
      "Inv3       M1                     6              244         0.982   \n",
      "           M2                     6              244         0.982   \n",
      "\n",
      "                       pollution_red_factor  expected_degradation  \n",
      "ag_level_2 ag_level_1                                              \n",
      "Inv1       M1                             1                 2.612  \n",
      "           M2                             1                 2.612  \n",
      "Inv2       M1                             1                 2.612  \n",
      "           M2                             1                 2.612  \n",
      "Inv3       M1                             1                 2.612  \n",
      "           M2                             1                 2.612  \n",
      "\n",
      "[6 rows x 21 columns]\n",
      "Timt taken for processing the data: 11.044237852096558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Feb/2022 11:34:13] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:14] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data summary:                                 Values\n",
      "Data Points Available         155.52 K\n",
      "Missing Data (%)                   0.0\n",
      "Outliers (%)                         0\n",
      "Temporal Resolution            10 Mins\n",
      "missing_data_post_sanitation       0.0\n",
      "dat_summary_value:  155.52 K\n",
      "###########Inside generate cards 1 - printing #######\n",
      "155.52 K total_data\n",
      "only str missing 0.0%\n",
      "only str outlier 0%\n",
      "outlier data: 0\n",
      "missing_data: 0\n",
      "####status Superb\n",
      "data summary:                                 Values\n",
      "Data Points Available         155.52 K\n",
      "Missing Data (%)                   0.0\n",
      "Outliers (%)                         0\n",
      "Temporal Resolution            10 Mins\n",
      "missing_data_post_sanitation       0.0\n",
      "dat_summary_value:  155.52 K\n",
      "Inside generate cards 2 - printing #######\n",
      "155.52 K total_data\n",
      "missing_data: Values    0.0\n",
      "Name: missing_data_post_sanitation, dtype: object%\n",
      "outlier data: 0 %\n",
      "####status Superb\n",
      "Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 181, in func2\n",
      "    return dcc.send_data_frame(inverter_data_sanitized.to_csv,\n",
      "UnboundLocalError: local variable 'inverter_data_sanitized' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Feb/2022 11:34:14] \"POST /_dash-update-component HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_current_graph:  ag_level_2                Inv1                  Inv2                  Inv3     \\\n",
      "ag_level_1                  M1         M2         M1         M2         M1      \n",
      "curve                        I  P  V    I  P  V    I  P  V    I  P  V    I  P   \n",
      "datetime                                                                        \n",
      "2020-04-01 00:00:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:10:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:20:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:30:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:40:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "...                        ... .. ..  ... .. ..  ... .. ..  ... .. ..  ... ..   \n",
      "2020-04-30 23:20:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-30 23:30:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-30 23:40:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-30 23:50:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-05-01 00:00:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "\n",
      "ag_level_2                               \n",
      "ag_level_1                     M2        \n",
      "curve                      V    I  P  V  \n",
      "datetime                                 \n",
      "2020-04-01 00:00:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:10:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:20:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:30:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:40:00+00:00  0  0.0  0  0  \n",
      "...                       ..  ... .. ..  \n",
      "2020-04-30 23:20:00+00:00  0  0.0  0  0  \n",
      "2020-04-30 23:30:00+00:00  0  0.0  0  0  \n",
      "2020-04-30 23:40:00+00:00  0  0.0  0  0  \n",
      "2020-04-30 23:50:00+00:00  0  0.0  0  0  \n",
      "2020-05-01 00:00:00+00:00  0  0.0  0  0  \n",
      "\n",
      "[4321 rows x 18 columns]\n",
      "update_current_graph:  ag_level_2                     Inv1                                    Inv2  \\\n",
      "ag_level_1                       M1                  M2                  M1   \n",
      "curve                             I    P    V         I    P    V         I   \n",
      "datetime                                                                      \n",
      "2020-04-01 07:20:00+00:00  0.101190   34  336  0.079108   39  493  0.103535   \n",
      "2020-04-01 07:30:00+00:00  0.190687   86  451  0.204301  114  558  0.197368   \n",
      "2020-04-01 07:40:00+00:00  0.227273  135  594  0.248804  156  627  0.234177   \n",
      "2020-04-01 07:50:00+00:00  0.353583  227  642  0.380655  244  641  0.354938   \n",
      "2020-04-01 08:00:00+00:00  0.407874  259  635  0.415504  268  645  0.397239   \n",
      "...                             ...  ...  ...       ...  ...  ...       ...   \n",
      "2020-04-30 19:50:00+00:00  0.213087  127  596  0.244556  146  597  0.226962   \n",
      "2020-04-30 20:00:00+00:00  0.235392  141  599  0.263333  158  600  0.244068   \n",
      "2020-04-30 20:10:00+00:00  0.219799  131  596  0.252931  151  597  0.220339   \n",
      "2020-04-30 20:20:00+00:00  0.186757  110  589  0.214890  127  591  0.192440   \n",
      "2020-04-30 20:30:00+00:00  0.099174   36  363  0.123570   54  437  0.086162   \n",
      "\n",
      "ag_level_2                                                   Inv3            \\\n",
      "ag_level_1                                 M2                  M1             \n",
      "curve                        P    V         I    P    V         I    P    V   \n",
      "datetime                                                                      \n",
      "2020-04-01 07:20:00+00:00   41  396  0.115207   50  434  0.080997   26  321   \n",
      "2020-04-01 07:30:00+00:00  105  532  0.196043  109  556  0.181395   78  430   \n",
      "2020-04-01 07:40:00+00:00  148  632  0.238655  142  595  0.219388  129  588   \n",
      "2020-04-01 07:50:00+00:00  230  648  0.379705  232  611  0.337481  217  643   \n",
      "2020-04-01 08:00:00+00:00  259  652  0.420195  258  614  0.384853  249  647   \n",
      "...                        ...  ...       ...  ...  ...       ...  ...  ...   \n",
      "2020-04-30 19:50:00+00:00  133  586  0.232014  129  556  0.227740  133  584   \n",
      "2020-04-30 20:00:00+00:00  144  590  0.255357  143  560  0.255102  150  588   \n",
      "2020-04-30 20:10:00+00:00  130  590  0.244165  136  557  0.225641  132  585   \n",
      "2020-04-30 20:20:00+00:00  112  582  0.205082  113  551  0.202773  117  577   \n",
      "2020-04-30 20:30:00+00:00   33  383  0.115979   45  388  0.072351   28  387   \n",
      "\n",
      "ag_level_2                                     \n",
      "ag_level_1                       M2            \n",
      "curve                             I    P    V  \n",
      "datetime                                       \n",
      "2020-04-01 07:20:00+00:00  0.068396   29  424  \n",
      "2020-04-01 07:30:00+00:00  0.185252  103  556  \n",
      "2020-04-01 07:40:00+00:00  0.235294  140  595  \n",
      "2020-04-01 07:50:00+00:00  0.369885  226  611  \n",
      "2020-04-01 08:00:00+00:00  0.419512  258  615  \n",
      "...                             ...  ...  ...  \n",
      "2020-04-30 19:50:00+00:00  0.263345  148  562  \n",
      "2020-04-30 20:00:00+00:00  0.290780  164  564  \n",
      "2020-04-30 20:10:00+00:00  0.267380  150  561  \n",
      "2020-04-30 20:20:00+00:00  0.230631  128  555  \n",
      "2020-04-30 20:30:00+00:00  0.145652   67  460  \n",
      "\n",
      "[2399 rows x 18 columns]\n",
      "update_current_graph:  ag_level_2                Inv1                  Inv2                  Inv3     \\\n",
      "ag_level_1                  M1         M2         M1         M2         M1      \n",
      "curve                        I  P  V    I  P  V    I  P  V    I  P  V    I  P   \n",
      "datetime                                                                        \n",
      "2020-04-01 00:00:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:10:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:20:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:30:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-01 00:40:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "...                        ... .. ..  ... .. ..  ... .. ..  ... .. ..  ... ..   \n",
      "2020-04-30 23:20:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-30 23:30:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-30 23:40:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-04-30 23:50:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "2020-05-01 00:00:00+00:00  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0  0  0.0  0   \n",
      "\n",
      "ag_level_2                               \n",
      "ag_level_1                     M2        \n",
      "curve                      V    I  P  V  \n",
      "datetime                                 \n",
      "2020-04-01 00:00:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:10:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:20:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:30:00+00:00  0  0.0  0  0  \n",
      "2020-04-01 00:40:00+00:00  0  0.0  0  0  \n",
      "...                       ..  ... .. ..  \n",
      "2020-04-30 23:20:00+00:00  0  0.0  0  0  \n",
      "2020-04-30 23:30:00+00:00  0  0.0  0  0  \n",
      "2020-04-30 23:40:00+00:00  0  0.0  0  0  \n",
      "2020-04-30 23:50:00+00:00  0  0.0  0  0  \n",
      "2020-05-01 00:00:00+00:00  0  0.0  0  0  \n",
      "\n",
      "[4321 rows x 18 columns]\n",
      "update_current_graph: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\dash.py\", line 1344, in dispatch\n",
      "    response.set_data(func(*args, outputs_list=outputs_list))\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\dash\\_callback.py\", line 151, in add_context\n",
      "    output_value = func(*func_args, **func_kwargs)  # %% callback invoked %%\n",
      "  File \"<ipython-input-10-5e2f34cf5610>\", line 154, in update_current_graph\n",
      "    fig1 = plot_data_analysis_graph(inverter_data, inverter_data_sanitized, inv_name=input_name,\n",
      "  File \"<ipython-input-2-d20fd730c906>\", line 43, in plot_data_analysis_graph\n",
      "    inverter, mppt = inv_name.split('-')[0], inv_name.split('-')[1]\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Feb/2022 11:34:14] \""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ag_level_2                     Inv1                                    Inv2  \\\n",
      "ag_level_1                       M1                  M2                  M1   \n",
      "curve                             I    P    V         I    P    V         I   \n",
      "datetime                                                                      \n",
      "2020-04-01 07:20:00+00:00  0.101190   34  336  0.079108   39  493  0.103535   \n",
      "2020-04-01 07:30:00+00:00  0.190687   86  451  0.204301  114  558  0.197368   \n",
      "2020-04-01 07:40:00+00:00  0.227273  135  594  0.248804  156  627  0.234177   \n",
      "2020-04-01 07:50:00+00:00  0.353583  227  642  0.380655  244  641  0.354938   \n",
      "2020-04-01 08:00:00+00:00  0.407874  259  635  0.415504  268  645  0.397239   \n",
      "...                             ...  ...  ...       ...  ...  ...       ...   \n",
      "2020-04-30 19:50:00+00:00  0.213087  127  596  0.244556  146  597  0.226962   \n",
      "2020-04-30 20:00:00+00:00  0.235392  141  599  0.263333  158  600  0.244068   \n",
      "2020-04-30 20:10:00+00:00  0.219799  131  596  0.252931  151  597  0.220339   \n",
      "2020-04-30 20:20:00+00:00  0.186757  110  589  0.214890  127  591  0.192440   \n",
      "2020-04-30 20:30:00+00:00  0.099174   36  363  0.123570   54  437  0.086162   \n",
      "\n",
      "ag_level_2                                                   Inv3            \\\n",
      "ag_level_1                                 M2                  M1             \n",
      "curve                        P    V         I    P    V         I    P    V   \n",
      "datetime                                                                      \n",
      "2020-04-01 07:20:00+00:00   41  396  0.115207   50  434  0.080997   26  321   \n",
      "2020-04-01 07:30:00+00:00  105  532  0.196043  109  556  0.181395   78  430   \n",
      "2020-04-01 07:40:00+00:00  148  632  0.238655  142  595  0.219388  129  588   \n",
      "2020-04-01 07:50:00+00:00  230  648  0.379705  232  611  0.337481  217  643   \n",
      "2020-04-01 08:00:00+00:00  259  652  0.420195  258  614  0.384853  249  647   \n",
      "...                        ...  ...       ...  ...  ...       ...  ...  ...   \n",
      "2020-04-30 19:50:00+00:00  133  586  0.232014  129  556  0.227740  133  584   \n",
      "2020-04-30 20:00:00+00:00  144  590  0.255357  143  560  0.255102  150  588   \n",
      "2020-04-30 20:10:00+00:00  130  590  0.244165  136  557  0.225641  132  585   \n",
      "2020-04-30 20:20:00+00:00  112  582  0.205082  113  551  0.202773  117  577   \n",
      "2020-04-30 20:30:00+00:00   33  383  0.115979   45  388  0.072351   28  387   \n",
      "\n",
      "ag_level_2                                     \n",
      "ag_level_1                       M2            \n",
      "curve                             I    P    V  \n",
      "datetime                                       \n",
      "2020-04-01 07:20:00+00:00  0.068396   29  424  \n",
      "2020-04-01 07:30:00+00:00  0.185252  103  556  \n",
      "2020-04-01 07:40:00+00:00  0.235294  140  595  \n",
      "2020-04-01 07:50:00+00:00  0.369885  226  611  \n",
      "2020-04-01 08:00:00+00:00  0.419512  258  615  \n",
      "...                             ...  ...  ...  \n",
      "2020-04-30 19:50:00+00:00  0.263345  148  562  \n",
      "2020-04-30 20:00:00+00:00  0.290780  164  564  \n",
      "2020-04-30 20:10:00+00:00  0.267380  150  561  \n",
      "2020-04-30 20:20:00+00:00  0.230631  128  555  \n",
      "2020-04-30 20:30:00+00:00  0.145652   67  460  \n",
      "\n",
      "[2399 rows x 18 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POST /_dash-update-component HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:15] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:16] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:16] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:37] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:37] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Feb/2022 11:34:38] \"GET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "@app.callback(\n",
    "    Output('input-container', 'children'),\n",
    "    Input('intermediate-value', 'data'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def update_output(jsonified_cleaned_data):\n",
    "    datasets = json.loads(jsonified_cleaned_data)\n",
    "    \n",
    "    array_info = deserialize_multiindex_dataframe(datasets['array_info'])\n",
    "    array_info.index.names = ['ag_level_2', 'ag_level_1']\n",
    "    all_inputs = array_info['input_name'].unique().tolist()\n",
    "\n",
    "    return dcc.Dropdown(\n",
    "        id='input-select',\n",
    "        options = [{'label': i, 'value': i} for i in all_inputs],\n",
    "        value = all_inputs[0],\n",
    "        persistence=True,\n",
    "        persisted_props=['value'],\n",
    "        persistence_type='session')\n",
    "        \n",
    "@app.callback(\n",
    "    Output('download-btn', 'n_clicks'),\n",
    "    Input('intermediate-value', 'data'),\n",
    ")              \n",
    "def download_data_button(jsonified_cleaned_data):\n",
    "    \n",
    "    datasets = json.loads(jsonified_cleaned_data)\n",
    "    inverter_data_sanitized = deserialize_multiindex_dataframe(datasets['inv_data_sani'])\n",
    "    inverter_data_sanitized.index = pd.to_datetime(inverter_data_sanitized.index)\n",
    "    inverter_data_sanitized.columns.names = ['ag_level_2', 'ag_level_1', 'curve']\n",
    "    inverter_data_sanitized.index.names = ['datetime']\n",
    "    \n",
    "    return html.Button(\"Download Sanitized Data\", id=\"btn-download-txt\",\n",
    "            style={'background-color': '#737373','color': '#FFFFFF', 'width':'280px',\n",
    "                   'border-radius': '8px',\n",
    "                    'fontSize':'105%'}),\n",
    "    dcc.Download(id='download_data')\n",
    "\n",
    "@app.callback(\n",
    "    Output('card-markdown', 'children'),\n",
    "    Input('upload-data', 'n_clicks'),\n",
    ")\n",
    "def markdown_text_cards(clicks):\n",
    "    time.sleep(15)\n",
    "    return dcc.Markdown(\n",
    "            children=(\n",
    "            \"\"\"\n",
    "            ##### **Note:**\n",
    "            ##### 1. The outliers denotes the datapoints removed from the dataset using plant's configuration.\n",
    "            ##### 2. The missing data is predicted with high accuracy using AI and ML to reduce data loss.\n",
    "            ##### 3. Data Status : If data availability > 99.5 % then its 'Superb'\n",
    "            ##### 3.1. If data availability > 98 % & and < 99.5 % then its 'Good'\n",
    "            ##### 3.2. If data availability > 95 % and < 98 % then its 'Normal' Else 'Poor' \n",
    "            \"\"\"),\n",
    "            style={'font-family': 'Roboto', 'fontcolor':'#333333', 'fontSize':'80%',\n",
    "                  'padding': '90px 0'}\n",
    "            ),\n",
    "\n",
    "@app.callback(\n",
    "    Output('bar_plot_missing', 'figure'),\n",
    "    Input('intermediate-value', 'data'),\n",
    ")\n",
    "def bar_plot_graph(jsonified_cleaned_data):\n",
    "    datasets = json.loads(jsonified_cleaned_data)\n",
    "    \n",
    "    array_info = deserialize_multiindex_dataframe(datasets['array_info'])\n",
    "    array_info.index.names = ['ag_level_2', 'ag_level_1']\n",
    "    \n",
    "    inverter_data_csky = deserialize_multiindex_dataframe(datasets['inv_data_csky'])\n",
    "    inverter_data_csky.index = pd.to_datetime(inverter_data_csky.index)\n",
    "    inverter_data_csky.columns.names = ['ag_level_2', 'ag_level_1', 'curve']\n",
    "    inverter_data_csky.index.names = ['datetime']\n",
    "    \n",
    "    figure=input_data_summary(array_info=array_info, df_in=inverter_data_csky)\n",
    "    return figure\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"markdown\", \"style\"),\n",
    "    [\n",
    "        Input(\"learn-more-button\", \"n_clicks\"), \n",
    "        Input(\"markdown_close\", \"n_clicks\")\n",
    "    ],\n",
    ")\n",
    "def update_click_output(button_click, close_click):\n",
    "    ctx = dash.callback_context\n",
    "    if ctx.triggered:\n",
    "        prop_id = ctx.triggered[0][\"prop_id\"].split(\".\")[0]\n",
    "        if prop_id == \"learn-more-button\":\n",
    "            return {\"display\": \"block\"}\n",
    "\n",
    "    return {\"display\": \"none\"}\n",
    "\n",
    "@app.callback(\n",
    "    Output('data-values-pre', 'children'),\n",
    "    Input('intermediate-value', 'data'),\n",
    ")\n",
    "def update_data_summary(jsonified_cleaned_data):\n",
    "\n",
    "    datasets = json.loads(jsonified_cleaned_data)\n",
    "    data_summary = pd.read_json(datasets['data_summary'], orient='index')\n",
    "    print('data summary:',data_summary)\n",
    "    print('dat_summary_value: ', data_summary.loc['Data Points Available']['Values'])\n",
    "    return generate_cards1(data_summary)\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('data-values-post', 'children'),\n",
    "    Input('intermediate-value', 'data'),\n",
    ")\n",
    "def update_data_summary(jsonified_cleaned_data):\n",
    "    datasets = json.loads(jsonified_cleaned_data)\n",
    "    data_summary = pd.read_json(datasets['data_summary'], orient='index')\n",
    "    print('data summary:',data_summary)\n",
    "    print('dat_summary_value: ', data_summary.loc['Data Points Available']['Values'])\n",
    "    return generate_cards2(data_summary)\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('current_graph', 'figure'),\n",
    "    Output('voltage_graph', 'figure'),\n",
    "    Output('irradiance_graph', 'figure'),\n",
    "    Input('intermediate-value', 'data'),\n",
    "    Input(\"input-select\", \"value\"),\n",
    ")\n",
    "def update_current_graph(jsonified_cleaned_data, input_name):\n",
    "    \"\"\"\n",
    "    :param input_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    datasets = json.loads(jsonified_cleaned_data)\n",
    "    \n",
    "    inverter_data = deserialize_multiindex_dataframe(datasets['inv_data'])\n",
    "    inverter_data.index = pd.to_datetime(inverter_data.index)\n",
    "    inverter_data.columns.names = ['ag_level_2', 'ag_level_1', 'curve']\n",
    "    inverter_data.index.names = ['datetime']\n",
    "    print('update_current_graph: ',inverter_data)\n",
    "    \n",
    "    inverter_data_sanitized = deserialize_multiindex_dataframe(datasets['inv_data_sani'])\n",
    "    inverter_data_sanitized.index = pd.to_datetime(inverter_data_sanitized.index)\n",
    "    inverter_data_sanitized.columns.names = ['ag_level_2', 'ag_level_1', 'curve']\n",
    "    inverter_data_sanitized.index.names = ['datetime']\n",
    "    print('update_current_graph: ',inverter_data_sanitized)\n",
    "    \n",
    "    irr_df = deserialize_multiindex_dataframe(datasets['irr_df'])\n",
    "    irr_df.index = pd.to_datetime(irr_df.index)\n",
    "    irr_df.columns.names = ['ag_level_2', 'ag_level_1', 'curve']\n",
    "    irr_df.index.names = ['datetime']\n",
    "    \n",
    "    meteo_data_csky = deserialize_multiindex_dataframe(datasets['meteo_data_csky'])\n",
    "    meteo_data_csky.index = pd.to_datetime(meteo_data_csky.index)\n",
    "    meteo_data_csky.columns.names = ['ag_level_2', 'ag_level_1', 'curve']\n",
    "    meteo_data_csky.index.names = ['datetime']\n",
    "\n",
    "    fig1 = plot_data_analysis_graph(inverter_data, inverter_data_sanitized, inv_name=input_name,\n",
    "                     variable='I', ylabel='Current (A)', title='')\n",
    "    fig2 = plot_data_analysis_graph(inverter_data, inverter_data_sanitized, inv_name=input_name,\n",
    "                     variable='V', ylabel='Voltage (V)', title='')\n",
    "    fig3 = plot_data_analysis_graph(irr_df, meteo_data_csky, inv_name=input_name, variable='G',\n",
    "                    ylabel='Irradiance (W/m\\u00b2)',title='')\n",
    "\n",
    "    return fig1, fig2, fig3\n",
    "\n",
    "        \n",
    "@app.callback(\n",
    "    Output(\"download_data\", \"data\"),\n",
    "    Input('upload-data', 'n_clicks'),\n",
    "    Input(\"btn-download-txt\", \"n_clicks\"),\n",
    "    Input('intermediate-value', 'data'),\n",
    "    prevent_initial_call=True,\n",
    ")\n",
    "def func2(clicks, n_clicks, jsonified_cleaned_data):\n",
    "    changed_id = [p['prop_id'] for p in dash.callback_context.triggered][0]\n",
    "\n",
    "    if 'btn-download-txt' in changed_id:\n",
    "        datasets = json.loads(jsonified_cleaned_data)\n",
    "        inverter_data_sanitized = deserialize_multiindex_dataframe(datasets['inv_data_sani'])\n",
    "        inverter_data_sanitized.index = pd.to_datetime(inverter_data_sanitized.index)#, origin = 'unix', unit = 'ms')\n",
    "        inverter_data_sanitized.columns.names = ['ag_level_2', 'ag_level_1', 'curve']\n",
    "        inverter_data_sanitized.index.names = ['datetime']\n",
    "        \n",
    "    return dcc.send_data_frame(inverter_data_sanitized.to_csv,\n",
    "                               \"sanitized_data.csv\")\n",
    "\n",
    "\n",
    "###########################################\n",
    "# # Page layout\n",
    "\n",
    "# app.layout = html.Div([\n",
    "#     dcc.Location(id = 'url', refresh = False),\n",
    "#     html.Div(id = 'page-content')\n",
    "# ])\n",
    "\n",
    "\n",
    "# @app.callback(\n",
    "#     Output('page-content', 'children'),\n",
    "#     Input('url', 'pathname')\n",
    "# )\n",
    "# def display_page(pathname):\n",
    "#     if pathname == '/connected_sites':\n",
    "#         return app_main()\n",
    "#     elif pathname == '/data_sanitation_dashboard':\n",
    "#         return data_app()\n",
    "#     elif pathname == '/learn_more':\n",
    "#         return learn_more()\n",
    "#     else:\n",
    "#         return Homepage()\n",
    "    \n",
    "\n",
    "#Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=False,port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
